{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45eacb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/miniconda3/envs/synergy/lib/python3.8/site-packages/pytorch_lightning/metrics/__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  rank_zero_deprecation(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from bertviz import model_view\n",
    "import jsonlines\n",
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from constants import ENTITY_START_MARKER, ENTITY_END_MARKER\n",
    "from data_loader import  DrugSynergyDataModule, make_fixed_length\n",
    "from model import RelationExtractor, load_model\n",
    "from preprocess import create_dataset\n",
    "from utils import construct_row_id_idx_mapping, set_seed, write_error_analysis_file\n",
    "\n",
    "from streamlit_single_relation_app import classify_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5571cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"/home/vijay/drug-synergy-models/checkpoints_with_relation_pretraining_seed_2024_multiclass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6cc864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer, metadata = load_model(checkpoint_path, output_attentions=True)\n",
    "tokenizer.add_tokens([ENTITY_START_MARKER, ENTITY_END_MARKER])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens([ENTITY_START_MARKER, ENTITY_END_MARKER])\n",
    "drugs = open(\"drugs.txt\").read().lower().split()\n",
    "tokenizer.add_tokens(drugs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84c9b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "message_text = \"The aims of this study were to determine the effects of (a) combining the epidermal growth factor\"+ \\\n",
    "\" receptor (EGFR) blocker (erlotinib) and the cyclooxygenase-2 inhibitor (celecoxib) on cell growth and apoptosis\"+ \\\n",
    "\" in human pancreatic cancer cell lines, (b) baseline EGFR expression on the potentiation of erlotinib-induced\"+ \\\n",
    "\" apoptosis by celecoxib, and (c) the effects of the combination on the expression of the COX-2, EGFR, HER-2/neu,\"+ \\\n",
    "\" and nuclear factor-kappaB (NF-kappaB). Baseline expression of EGFR was determined by Western blot analysis in\"+ \\\n",
    "\" five human pancreatic cancer cell lines. BxPC-3, PANC-1, and HPAC had high EGFR and MIAPaCa had low EGFR. Cells\"+ \\\n",
    "\" were grown in culture and treated with erlotinib (1 and 10 micromol/L), celecoxib (1 and 10 micromol/L), and the\"+ \\\n",
    "\" combination. Growth inhibition was evaluated using 3-(4,5-dimethylthiazol-2-yl)-2,5-diphenyltetrazolium bromide\"+ \\\n",
    "\" assay, and apoptosis was assayed by ELISA. Reverse transcriptase-PCR was used to evaluate COX-2 and EGFR mRNA.\"+ \\\n",
    "\" EGFR, COX-2, and HER-2/neu expression was determined by Western immunoblotting. Electrophoretic mobility shift\"+ \\\n",
    "\" assay was used to evaluate NF-kappaB activation. Growth inhibition and apoptosis were significantly ( P < 0.05 )\"+ \\\n",
    "\" higher in BxPC-3 , HPAC , and PANC-1 cells treated with <<m>> celecoxib <</m>> and <<m>> erlotinib <</m>> than cells \"+ \\\n",
    "\" treated with either\"+ \\\n",
    "\" celecoxib or erlotinib or cisplatin . However, no potentiation in growth inhibition or apoptosis was observed in the MIAPaCa\"+ \\\n",
    "\" cell line with low expression of the EGFR. Significant down-regulation of COX-2 and EGFR expression was observed\"+ \\\n",
    "\" in the BxPC-3 and HPAC cells treated with the combination of erlotinib (1 micromol/L) and celecoxib (10 micromol/L)\"+ \\\n",
    "\" compared with celecoxib- or erlotinib-treated cells. celecoxib significantly down-regulated HER-2/neu expression in\"+ \\\n",
    "\" BxPC-3 and HPAC cell lines. Significant inhibition of NF-kappaB activation was observed in BxPC-3 and HPAC cell\"+ \\\n",
    "\" lines treated with erlotinib and celecoxib. (a) celecoxib can potentiate erlotinib-induced growth inhibition and\"+ \\\n",
    "\" apoptosis in pancreatic cell lines, (b) high baseline EGFR expression is a predictor of this potentiation, and (c)\"+ \\\n",
    "\" the down-regulation of EGFR, COX-2, and HER-2/neu expression and NF-kappaB inactivation contributes to the\"+ \\\n",
    "\" potentiation of erlotinib by celecoxib, suggesting an alternative to cisplatin.\"\n",
    "'''\n",
    "\n",
    "message_text = \"Various cutaneous side-effects have been reported with anti-melanoma systemic therapies. \"+ \\\n",
    "\"This study investigated the changes in melanocytic lesion pigmentation in patients on four different therapies.\"+ \\\n",
    "\" ### methods We analysed the serial dermatoscopic photographs of atypical melanocytic lesions taken from patients\"+ \\\n",
    "\" with advanced metastatic melanoma on four different systemic therapies ( selective BRAF-inhibitor monotherapy ,\"+ \\\n",
    "\" <<m>> dabrafenib <</m>> combined with <<m>> trametinib <</m>> [ D&T ] , anti-programmed cell death protein 1 [ \"+ \\\n",
    "\"anti-PD1 ] therapies , and anti-PD1 combined with ipilimumab ) seen from February 2013 to May 2016 . We compared \"+ \\\n",
    "\"these changes with the melanocytic lesions of 10 control patients. ### results In the control group, 19% of naevi \"+ \\\n",
    "\"lightened, 64% did not change and 17% darkened. Only the BRAF inhibitor group showed more darkened lesions than \"+ \\\n",
    "\"controls (37%, P < 0.001). Meanwhile, there were more lightened naevi in the D&T therapy group (86%, P < 0.001) \"+ \\\n",
    "\"as well as the anti-PD1 and ipilimumab groups (59%, P < 0.001) than controls. Patients on anti-PD1 monotherapy \"+ \\\n",
    "\"had more lightened (49%) and fewer darkened naevi (9%) than controls, but differences were not significant. ###\"+ \\\n",
    "\" conclusions Our study showed that different anti-melanoma systemic therapies have different effects on the \"+ \\\n",
    "\"pigmentation of melanocytic lesions. BRAF inhibitor may have the propensity to cause darkening while D&T therapy\"+ \\\n",
    "\" and anti-PD1 caused lightening compared with controls. The findings emphasise the importance of regular \"+ \\\n",
    "\"dermatological monitoring in specialised clinics for patients on anti-melanoma systemic therapy. Clinicians \"+ \\\n",
    "\"should expect changes in the global pigmentation of melanocytic lesions but be suspicious of lesions with \"+ \\\n",
    "\"structural changes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f277a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode(message_text, return_tensors='pt')\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9747cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = make_fixed_length(tokens, metadata.max_seq_length, padding_value = \"<PAD>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b2015d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vijay/drug-synergy-models/streamlit_single_relation_app.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilities = torch.nn.functional.softmax(logits)[0]\n"
     ]
    }
   ],
   "source": [
    "model_output, attention = classify_message(message_text, model, tokenizer, metadata.max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d686de29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_keep_nth_head(attention_layer, head_idxs, seq_length):\n",
    "    return attention_layer[:, head_idxs, :seq_length, :seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660d36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_pruned = [only_keep_nth_head(attention[i], list(range(12)), len(tokens)) for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afd5c4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing attention head 0 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "emphasis (338)\t\t\t0.014879440888762474\n",
      "clinics (351)\t\t\t0.014471146278083324\n",
      "study (288)\t\t\t0.014462144114077091\n",
      "special (349)\t\t\t0.010471430607140064\n",
      "changes (127)\t\t\t0.010239566676318645\n",
      "therapies (296)\t\t\t0.009719575755298138\n",
      "systemic (295)\t\t\t0.009239290840923786\n",
      "[CLS] (0)\t\t\t0.00855463556945324\n",
      "dermatol (344)\t\t\t0.008067985996603966\n",
      "showed (289)\t\t\t0.00772210443392396\n",
      "\n",
      "\n",
      "\n",
      "Showing attention head 1 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "fewer (264)\t\t\t0.09122643619775772\n",
      "propensity (314)\t\t0.026614848524332047\n",
      "conclusions (286)\t\t0.02536991611123085\n",
      "monotherapy (67)\t\t0.020404068753123283\n",
      "monotherapy (254)\t\t0.020290911197662354\n",
      "serial (41)\t\t\t0.018741831183433533\n",
      "##inib (80)\t\t\t0.016001060605049133\n",
      "[SEP] (383)\t\t\t0.015830054879188538\n",
      "therapies (61)\t\t\t0.014698452316224575\n",
      "controls (275)\t\t\t0.013395760208368301\n",
      "\n",
      "\n",
      "\n",
      "Showing attention head 2 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "[CLS] (0)\t\t\t0.02521866373717785\n",
      "[SEP] (383)\t\t\t0.014478336088359356\n",
      "for (352)\t\t\t0.011963923461735249\n",
      ") (262)\t\t\t\t0.00919544231146574\n",
      "on (354)\t\t\t0.009083102457225323\n",
      "and (263)\t\t\t0.009042955003678799\n",
      "in (27)\t\t\t\t0.008987159468233585\n",
      "with (9)\t\t\t0.008701781742274761\n",
      "various (1)\t\t\t0.008636648766696453\n",
      "that (290)\t\t\t0.008235706947743893\n",
      "\n",
      "\n",
      "\n",
      "Showing attention head 3 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "therapies (61)\t\t\t0.06993167102336884\n",
      "therapies (101)\t\t\t0.036764100193977356\n",
      "monotherapy (67)\t\t0.033253610134124756\n",
      "therapies (32)\t\t\t0.02218327485024929\n",
      "<<m>> (77)\t\t\t0.022171536460518837\n",
      "lesion (24)\t\t\t0.022099299356341362\n",
      "programmed (90)\t\t\t0.021685918793082237\n",
      "therapies (14)\t\t\t0.021666185930371284\n",
      "four (58)\t\t\t0.019976060837507248\n",
      "atypical (46)\t\t\t0.019677307456731796\n",
      "\n",
      "\n",
      "\n",
      "Showing attention head 4 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "monotherapy (67)\t\t0.21225835382938385\n",
      "<<m>> (69)\t\t\t0.055961865931749344\n",
      "inhibitor (66)\t\t\t0.04881024733185768\n",
      "monotherapy (254)\t\t0.04120303690433502\n",
      "<<m>> (77)\t\t\t0.03728530555963516\n",
      "four (58)\t\t\t0.03583655506372452\n",
      "pd (252)\t\t\t0.028656039386987686\n",
      "systemic (60)\t\t\t0.027498463168740273\n",
      "fewer (264)\t\t\t0.02390357106924057\n",
      "metastatic (55)\t\t\t0.021564986556768417\n",
      "\n",
      "\n",
      "\n",
      "Showing attention head 5 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "monotherapy (67)\t\t0.25853243470191956\n",
      "<<m>> (69)\t\t\t0.2023553103208542\n",
      "inhibitor (66)\t\t\t0.09012295305728912\n",
      "<</m>> (74)\t\t\t0.05792950466275215\n",
      "pd (252)\t\t\t0.047909241169691086\n",
      "four (58)\t\t\t0.03703053295612335\n",
      "<<m>> (77)\t\t\t0.02835043892264366\n",
      "00 (242)\t\t\t0.027026262134313583\n",
      "##ra (71)\t\t\t0.02509765513241291\n",
      "combined (75)\t\t\t0.01761516183614731\n",
      "\n",
      "\n",
      "\n",
      "Showing attention head 6 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "<<m>> (69)\t\t\t0.7347148060798645\n",
      "##ra (71)\t\t\t0.17171329259872437\n",
      "dab (70)\t\t\t0.02939932607114315\n",
      "<<m>> (77)\t\t\t0.008118625730276108\n",
      "pd (252)\t\t\t0.007284828461706638\n",
      "<</m>> (74)\t\t\t0.0064574237912893295\n",
      "monotherapy (67)\t\t0.004590038675814867\n",
      "##fen (72)\t\t\t0.0032209386117756367\n",
      "- (251)\t\t\t\t0.003079074202105403\n",
      ", (68)\t\t\t\t0.0028109552804380655\n",
      "\n",
      "\n",
      "\n",
      "Showing attention head 7 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "atypical (46)\t\t\t0.26736217737197876\n",
      "monotherapy (67)\t\t0.06671199947595596\n",
      "serial (41)\t\t\t0.04742008075118065\n",
      "systemic (60)\t\t\t0.03766831010580063\n",
      "monotherapy (254)\t\t0.030217846855521202\n",
      "therapies (61)\t\t\t0.016701484099030495\n",
      "various (1)\t\t\t0.015817666426301003\n",
      "expect (363)\t\t\t0.01453938614577055\n",
      "systemic (295)\t\t\t0.012898819521069527\n",
      "metastatic (55)\t\t\t0.012619678862392902\n",
      "\n",
      "\n",
      "\n",
      "Showing attention head 8 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "monotherapy (67)\t\t0.09800896793603897\n",
      "monotherapy (254)\t\t0.05996652692556381\n",
      "serial (41)\t\t\t0.05005505308508873\n",
      "metastatic (55)\t\t\t0.038838453590869904\n",
      "lesion (24)\t\t\t0.03707592189311981\n",
      "pigment (25)\t\t\t0.03162422776222229\n",
      "braf (64)\t\t\t0.025691233575344086\n",
      "pd (252)\t\t\t0.02152079902589321\n",
      "pigment (302)\t\t\t0.020799512043595314\n",
      "atypical (46)\t\t\t0.02038010209798813\n",
      "\n",
      "\n",
      "\n",
      "Showing attention head 9 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "[CLS] (0)\t\t\t0.037680212408304214\n",
      "dermatol (344)\t\t\t0.0212024487555027\n",
      "monotherapy (67)\t\t0.015878241509199142\n",
      "that (290)\t\t\t0.014558780007064342\n",
      "[SEP] (383)\t\t\t0.012754958122968674\n",
      "while (319)\t\t\t0.012465961277484894\n",
      "propensity (314)\t\t0.0117125753313303\n",
      "combined (75)\t\t\t0.011673860251903534\n",
      "monotherapy (254)\t\t0.011110872961580753\n",
      "anti (96)\t\t\t0.010374713689088821\n",
      "\n",
      "\n",
      "\n",
      "Showing attention head 10 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "melanoma (56)\t\t\t0.09354110807180405\n",
      "monotherapy (67)\t\t0.052992500364780426\n",
      "various (1)\t\t\t0.04797455295920372\n",
      "conclusions (286)\t\t0.03415293246507645\n",
      "photographs (44)\t\t0.032929155975580215\n",
      "melanoma (294)\t\t\t0.028317783027887344\n",
      "braf (64)\t\t\t0.023314720019698143\n",
      "combined (75)\t\t\t0.018457934260368347\n",
      "therapies (61)\t\t\t0.01816864125430584\n",
      "fewer (264)\t\t\t0.017592353746294975\n",
      "\n",
      "\n",
      "\n",
      "Showing attention head 11 in transformer layer 0\n",
      "Query entity: entity marker before \"dab\" (69)\n",
      "===================================================\n",
      "Subword\t\t\t|\tAttention weight\n",
      "---------------------------------------------------\n",
      "<<m>> (69)\t\t\t0.5868487358093262\n",
      "monotherapy (67)\t\t0.17670795321464539\n",
      ", (68)\t\t\t\t0.11147765070199966\n",
      "dab (70)\t\t\t0.013192315585911274\n",
      "inhibitor (66)\t\t\t0.012366173788905144\n",
      "00 (242)\t\t\t0.01045630220323801\n",
      ". (247)\t\t\t\t0.006967837922275066\n",
      ". (282)\t\t\t\t0.006177688017487526\n",
      "<</m>> (74)\t\t\t0.005856189876794815\n",
      "- (65)\t\t\t\t0.005087424535304308\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_top_attended_words(attention_tensors, query_token_idx, layer_idx, attn_head_idx, num_words_to_show=10):\n",
    "    top_self_attentions = np.argsort(-attention_tensors[layer_idx][0][attn_head_idx][query_token_idx].detach())[:num_words_to_show]\n",
    "    for other_token_idx in top_self_attentions:\n",
    "        word = f\"{tokens[other_token_idx]} ({other_token_idx})\"\n",
    "        if len(word) <= 7:\n",
    "            num_tabs = 4\n",
    "        elif len(word) >= 16:\n",
    "            num_tabs = 2\n",
    "        else:\n",
    "            num_tabs = 3\n",
    "        tabs = \"\".join([\"\\t\" for _ in range(num_tabs)])\n",
    "        print(f\"{tokens[other_token_idx]} ({other_token_idx}){tabs}{attention_pruned[layer_idx][0][attn_head_idx][start][other_token_idx]}\")\n",
    "\n",
    "TRANSFORMER_LAYER_IDX = 0\n",
    "ATTENTION_HEAD_IDX = 4\n",
    "start_token_idxs = [i for i, v in enumerate(tokens) if v == \"<<m>>\"]\n",
    "for attention_head in range(12):\n",
    "    start = start_token_idxs[0]\n",
    "    print(f\"Showing attention head {attention_head} in transformer layer {TRANSFORMER_LAYER_IDX}\")\n",
    "    print(f\"Query entity: entity marker before \\\"{tokens[start+1]}\\\" ({start})\")\n",
    "    print(\"===================================================\")\n",
    "    print(f\"Subword\\t\\t\\t|\\tAttention weight\\n---------------------------------------------------\")\n",
    "    print_top_attended_words(attention_pruned, start, TRANSFORMER_LAYER_IDX, attention_head)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b505b04",
   "metadata": {},
   "source": [
    "# model_view(attention_pruned, tokens[:metadata.max_seq_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86f1742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
